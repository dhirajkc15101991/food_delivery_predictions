Leading_Food_Delivery_Time_Prediction
==============================

Build Ml-Project for predicting the delivery time of order

Project Organization
------------

    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ Makefile           <- Makefile with commands like `make data` or `make train`
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
    ‚îÇ
    ‚îú‚îÄ‚îÄ docs               <- A default Sphinx project; see sphinx-doc.org for details
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    ‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.
    ‚îÇ                         `1.0-jqp-initial-data-exploration`.
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
    ‚îÇ
    ‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    ‚îÇ                         generated with `pip freeze > requirements.txt`
    ‚îÇ
    ‚îú‚îÄ‚îÄ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data           <- Scripts to download or generate data
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ make_dataset.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ features       <- Scripts to turn raw data into features for modeling
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ build_features.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models         <- Scripts to train models and then use trained models to make
    ‚îÇ   ‚îÇ   ‚îÇ                 predictions
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ predict_model.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ train_model.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualization  <- Scripts to create exploratory and results oriented visualizations
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ visualize.py
    ‚îÇ
    ‚îî‚îÄ‚îÄ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>

 conda create --name fiid_delivery_env_3_11 python=3.11
 source activate fiid_delivery_env_3_11 



Steps for dvc setup:

1.dvc init

It creates a .dvc/ folder, which contains DVC‚Äôs internal config and state files.

.dvc/config:
This is the configuration file for DVC. It stores default settings such as:
[core]
    remote = storage
You can configure remotes, cache location, and more here.

.dvc/.gitignore:
This file ensures Git ignores DVC's internal files that shouldn‚Äôt be committed (like cache or temp data).
It usually contains:
/cache/
===============================================
‚ö†Ô∏è Why are .dvc/ files being tracked by Git?
Even though DVC manages large data files separately, some DVC files are meant to be committed, especially:

.dvc/config ‚Üí tracks DVC settings (required)

.dvc/.gitignore ‚Üí tells Git to ignore large file caches

.dvcignore ‚Üí tells DVC what to ignore (like temp files)

These files are text metadata and help others reproduce your DVC setup ‚Äî that's why Git tracks them.

‚úÖ They are supposed to be committed to Git. Only bulky files (like datasets, model binaries) are stored outside Git.


For tracking the data by dvc:
=============================
First remove the /data/ from git ignore - allow git to track .
we wil track data by dvc and will allow git to use the metadata to track:
dvc add data/raw/swiggy.csv
it will create two file inside the raw .gitignore and swiggy.csv.dvc - whom dvc will track


dvc.yaml - we will write steps for our pipeline

params.yaml - we will define the parameters

dvc dag --it will make the dag through dvc.yaml file - to see our flow we can see

dvc repro


=======

dvc init
git add .dvc .gitignore
git commit -m "Initialize DVC"

#setup local remote for dvc
mkdir -p dvc_storage
dvc remote add -d local_remote ./dvc_storage



============
issue fix SSL_CERT_FILE - IN ACCESING DAGS HUB THIS ISSUE HAPPENS - AS PATH DIFFERENCE OOCUS

export SSL_CERT_FILE="C:/Users/dkcra/miniconda3/envs/fiid_delivery_env_3_11/Library/ssl/cacert.pem"
set SSL_CERT_FILE=C:\Users\dkcra\miniconda3\envs\fiid_delivery_env_3_11\Library\ssl\cacert.pem


==============================================================
Mlflow:-in production setup

mlflow server \
  --backend-store-uri postgresql://username:password@host:port/mlflowdb \
  --default-artifact-root s3://mlflow-artifacts-bucket/artifacts/ \
  --host 0.0.0.0 \
  --port 5000


  backend-store-uri:
  MLflow automatically creates all required tables in the backend DB when you first start it with a new database.

These tables handle:

Experiments
Runs
Metrics
Parameters
Tags
etc.


It will check the DB:

If the mlflowdb is empty ‚Üí it will create the required schema/tables.

üîπ For S3:

It stores artifact files in the s3://mlflow-artifacts-bucket/artifacts/ path.

Filenames/folder structures are handled by MLflow using the run ID.
=====================================================


python --version

python -m venv myenv

source myenv/Scripts/activate


===========
add remote

dvc remote list

dvc remote add -d myremote s3://food-delivery-time-pred-project-Dhiraj/prefix

dvc status

dvc push


----
Note: So now when we do the git repo then we can have dvc pull to get all the files





